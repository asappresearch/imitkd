generation: .

---
!Experiment

name: wmt-seqkd-full

resources:
  data: /Users/alexanderlin/Desktop/imitkd/data/wmt/wmt16_en_de.tar.gz
  base_dir: /Users/alexanderlin/Desktop/imitkd/data/wmt/seqkd_dir


pipeline:

  seqKD: !generation.SeqKDRunner # Can skip if teacher-generated dataset already exists
    dataset: !generation.WMT16Dataset
      file: !@ data
      use_bpe_tokenized: True
      transform:
        src: !generation.PretrainedFairseqTextField
          alias: 'transformer.wmt16.en-de'
          use_tgt_dict: False
        tgt: !generation.PretrainedFairseqLMField
          alias: 'transformer.wmt16.en-de'
          use_tgt_dict: True
      cache: True
    sampler: !generation.BaseSamplerWithFilter
      max_seq_len: 101
      batch_size: 256
      drop_last: False
      pad_index: !@ seqKD[dataset].tgt.pad_idx
    translator: !generation.PretrainedFairseqSeq2SeqTranslator
      alias: 'transformer.wmt16.en-de'
      tgt_sos_idx: !@ seqKD[dataset].tgt.sos_idx
      tgt_eos_idx: !@ seqKD[dataset].tgt.eos_idx
      max_seq_len: 100
      beam_size: 5
    base_dir: !@ base_dir
    train_file: 'train_file.p'
    val_file: 'val_file.p'
    test_file: 'test_file.p'
    others_file: 'others_file.p'
    batch_saving: 2000

  train: !generation.Seq2SeqDistillTrainer
    dataset: !generation.TensorDataset
      base_dir: !@ base_dir
      train_file: ['0train_file.p', '1train_file.p', '2train_file.p', '3train_file.p', '4train_file.p', '5train_file.p', '6train_file.p', '7train_file.p', '8train_file.p']
      val_file: 'val_file.p'
      test_file: 'test_file.p'
      others_file: 'others_file.p'
      columns: ['src', 'tgt_context', 'tgt_words']
    model: !generation.Seq2Seq
      src_embedding: !torch.Embedding
        num_embeddings: !@ train[dataset].src_vocab_size
        embedding_dim: 256
      tgt_embedding: !torch.Embedding
        num_embeddings: !@ train[dataset].tgt_vocab_size
        embedding_dim: !@ train[model][src_embedding][embedding_dim]
      embedding_dropout: 0.1
      encoder: !generation.RNNEncoder
        input_size: !@ train[model][src_embedding][embedding_dim]
        n_layers: 4
        hidden_size: 1024
        projection_size: 256
        rnn_type: 'sru'
        dropout: !@ train[model][embedding_dropout]
        bidirectional: True
        layer_norm: True
      decoder: !generation.RNNDecoder
        input_size: !@ train[model][tgt_embedding][embedding_dim]
        n_layers: !@ train[model][encoder][n_layers]
        hidden_size: 2048
        projection_size: !@ train[model][encoder][projection_size]
        rnn_type: !@ train[model][encoder][rnn_type]
        dropout: !@ train[model][encoder][dropout]
        attention: !generation.DotProductAttention
        activation: !torch.Tanh
        layer_norm: True
      output_layer: !generation.Sequential
        proj: !torch.Linear
          in_features: !@ train[model][decoder][hidden_size]
          out_features: !@ train[model][decoder][projection_size]
        final: !torch.Linear
          in_features: !@ train[model][output_layer][proj][out_features]
          out_features: !@ train[dataset].tgt_vocab_size
      dropout: !@ train[model][embedding_dropout]
      weight_tying:
        output_layer.final: tgt_embedding
        src_embedding: tgt_embedding
      src_padding_idx: 1
      tgt_padding_idx: 1
    translator: !generation.GreedyTranslator
      tgt_sos_idx: 2
      tgt_eos_idx: 2
      max_seq_len: 100
    train_sampler: !generation.BaseSamplerWithFilter
      pad_index: 1
      max_seq_len: 100
      batch_size: 512
      drop_last: True
    val_sampler: !generation.BaseSamplerWithFilter
      pad_index: 1
      max_seq_len: 100
      batch_size: 128
      drop_last: False
    loss_fn: !torch.CrossEntropyLoss
    metric_fn: !Perplexity
    optimizer: !torch.Adam
      lr: 5e-2
      params: !@ train[model].trainable_params
    max_steps: 60
    epoch_per_step: 0.25
    batches_per_iter: 1
    batches_per_samp: 1
    lower_is_better: True
    max_grad_norm: 2
    bleu_fn: !generation.Bleu
      vocab: !@ train[dataset].tgt_vocab
      specials: ['<pad>', '</s>']
    iter_scheduler: !NoamScheduler
      optimizer: !@ train[optimizer]
      warmup: 4000
      d_model: 1
    teacher: !generation.PretrainedFairseqSeq2Seq
      alias: 'transformer.wmt16.en-de'
    scheduler_type: 'ones' # Take all context from teacher-generated dataset
    top_k: -1 # Use full teacher softmax

  eval: !Evaluator
    dataset: !generation.TensorDataset
      base_dir: !@ base_dir
      train_file: ['0train_file.p', '1train_file.p', '2train_file.p', '3train_file.p', '4train_file.p', '5train_file.p', '6train_file.p', '7train_file.p', '8train_file.p']
      val_file: 'val_file.p'
      test_file: 'test_file.p'
      others_file: 'others_file.p'
      columns: ['src', 'tgt_context', 'tgt_words']
    model: !generation.BeamSearchTranslator
      model: !@ train[model]
      tgt_sos_idx: 2
      tgt_eos_idx: 2
      max_seq_len: 100
      beam_size: !g [1, 5]
    metric_fn: !generation.Bleu
      vocab: !@ eval[dataset].tgt_vocab
      specials: ['<pad>', '</s>']
    eval_data: 'test'
    eval_sampler: !BaseSampler
      batch_size: 128
      drop_last: False
      pad_index: 1

devices:
  gpu: 4
